
<h1 align="center">Jailbreak in pieces: Compositional Adversarial Attacks on Multi-Modal Language Models</h1>
    <p align='center' style="text-align:center;font-size:1em;">
    <a href="https://erfanshayegani.github.io/" target="_blank" style="text-decoration: none;">Erfan Shayegani</a>, <a href="https://yuedong.us/" target="_blank" style="text-decoration: none;">Yue Dong</a>, <a href="https://www.cs.ucr.edu/~nael/" target="_blank" style="text-decoration: none;">Nael Abu-Ghazaleh</a>  
    </p>
<h2 align="center">üî• ICLR 2024 Spotlight - üèÜ Best Paper Award SoCal NLP 2023</h2>

$${\color{red}\text{\textbf{Warning: This repo has harmful content!}}}$$
[Jailbreak in pieces: Compositional Adversarial Attacks on Multi-Modal Language Models](https://arxiv.org/abs/2307.14539)
<div align="center">
<img width="100%" alt="JBPieces" src="images/JBPieces_logo.png">
</div>


## 1. Overview
Jailbreak In Pieces was the first to discover the ``Cross-Modality Safety Alignment'' Phenomemon in Vision-Language Models (VLMs) as a result of adding extra modalities to LLMs.