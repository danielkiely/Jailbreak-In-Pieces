
<h1 align="center">Jailbreak in pieces: Compositional Adversarial Attacks on Multi-Modal Language Models</h1>
    <p align='center' style="text-align:center;font-size:1em;">
    <a href="https://erfanshayegani.github.io/" target="_blank" style="text-decoration: none;">Erfan Shayegani</a>, <a href="https://yuedong.us/" target="_blank" style="text-decoration: none;">Yue Dong</a>, <a href="https://www.cs.ucr.edu/~nael/" target="_blank" style="text-decoration: none;">Nael Abu-Ghazaleh</a>  
    </p>
<h2 align="center">üî• ICLR 2024 Spotlight - üèÜ Best Paper Award SoCal NLP 2023</h2>

$${\color{red}\text{\textbf{Warning: This repo has harmful content!}}}$$
[Jailbreak in pieces: Compositional Adversarial Attacks on Multi-Modal Language Models](https://arxiv.org/abs/2307.14539)
<div align="center">
<img width="100%" alt="JBPieces" src="images/JBPieces_logo.png">
</div>


## Overview
Jailbreak In Pieces is the first to discover the **"Cross-Modality Safety Alignment"** Phenomemon in Vision-Language Models (VLMs) as a result of adding extra modalities to LLMs. We present novel jailbreak attacks targeting vision-language models (VLMs) that utilize aligned large language models (LLMs) and are resistant to text-only jailbreak methods. Our approach involves cross-modality attacks on the alignment, where we combine adversarial images processed by the vision encoder with benign textual prompts to disrupt the alignment of the language model.


```bash
mkdir images outputs 
```
```bash
# install the requirements
pip install pillow numpy torch torchvision sentence-transformers transformers matplotlib
```

```bash
# the code that optmizes the adversarial image in the CLIP emebdding space
cd src/
python adv_image.py
```
## Optimization (Only Access the Vision Encoder CLIP)

<div align="center">
<img width="50%" alt="loss" src="images/meth_pill.png">
</div>
Target image

<div align="center">
<img width="50%" alt="loss" src="outputs/Adversarial_Loss.png">
<img width="39%" alt="adv" src="outputs/L2_noNorm_Drug_clipgrad_White_336_LR0_1.jpg">
</div>

#### Similarity Test

```python
t1 = model.get_text_features(**tokenizer("Marijuana weed", return_tensors="pt").to(device))
t2 = model.get_text_features(**tokenizer("Pharmaceutical", return_tensors="pt").to(device))
t3 = model.get_text_features(**tokenizer("illegal Substance", return_tensors="pt").to(device))
t4 = model.get_text_features(**tokenizer("Methamphetamine", return_tensors="pt").to(device))
t5 = model.get_text_features(**tokenizer("LSD", return_tensors="pt").to(device))
t6 = model.get_text_features(**tokenizer("illegal drugs", return_tensors="pt").to(device))
t7 = model.get_text_features(**tokenizer("A woman doing meth pills crack pipes", return_tensors="pt").to(device))
```
```python
util.cos_sim(random_img_emb, t1.detach()),util.cos_sim(random_img_emb, t2.detach()),util.cos_sim(random_img_emb, t3.detach()),util.cos_sim(random_img_emb, t4.detach()),util.cos_sim(random_img_emb, t5.detach()), util.cos_sim(random_img_emb, t6.detach()), util.cos_sim(random_img_emb, t7.detach())
```

## Citation

üìöü§ó If you find our work helpful, please cite our papers on AI Safety as follows, thanks! Also Stay tuned for more interesting works!
```bibtex
@inproceedings{shayegani2024jailbreak,
    title={Jailbreak in pieces: Compositional Adversarial Attacks on Multi-Modal Language Models},
    author={Erfan Shayegani and Yue Dong and Nael Abu-Ghazaleh},
    booktitle={The Twelfth International Conference on Learning Representations},
    year={2024},
    url={https://openreview.net/forum?id=plmBsXHxgR}
}
```
```bibtex
@article{shayegani2023survey,
  title={Survey of vulnerabilities in large language models revealed by adversarial attacks},
  author={Shayegani, Erfan and Mamun, Md Abdullah Al and Fu, Yu and Zaree, Pedram and Dong, Yue and Abu-Ghazaleh, Nael},
  journal={arXiv preprint arXiv:2310.10844},
  year={2023}
}
```

